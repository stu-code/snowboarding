[{"kind":1,"language":"markdown","value":"# Extract Data\r\n\r\nThis notebook will extract and transform GPS and biometric data collected while snowboarding. It will extract GPS and heart rate data collected from the Slopes app and from FitBit data exported from a Pixel Watch 2. There are two steps to process all of the data:\r\n\r\n1. Python to extract the data\r\n2. SAS for data merging and final data transformation\r\n\r\n### Input Data\r\n- __GPS__ - GPS data in .gpx format and GPS metadata in .slopes format\r\n- __Heart Rate__ - Heart rate data in .json format exported from FitBit\r\n\r\n### Folder Structure\r\n\r\nTo run this notebook successfully, it must be in a location with this folder structure on the SAS Viya server in a persistent volume:\r\n\r\n```\r\n[root]\r\n   |\r\n   |-----[stage]\r\n   |-----[snowboarding]\r\n               |--------- extract_snowboard_data.ipynb\r\n               |--------- [data]\r\n                            |----- [gps]\r\n                            |----- [hr]\r\n```","outputs":[]},{"kind":2,"language":"python","value":"import pandas as pd\r\nimport json\r\nimport os\r\nimport xml.etree.ElementTree as ET\r\nimport requests\r\nfrom zipfile import ZipFile\r\nfrom dateutil import parser","outputs":[]},{"kind":1,"language":"markdown","value":"Expected to be in this current working directory with three folders:\r\n- `/data` - Input data location\r\n- `/data/hr` - Heart rate data location\r\n- `/data/gps` - GPS data location","outputs":[]},{"kind":2,"language":"python","value":"data_loc = '.../snowboarding/data' # Put location of /data here. This must be on a persistent volume on the Viya server.\r\nhr_data  = os.path.join(data_loc, 'hr')\r\ngps_data = os.path.join(data_loc, 'gps')","outputs":[]},{"kind":1,"language":"markdown","value":"## Read Heart Rate Data\r\n\r\nHeart rate data is in JSON format. Variables include:\r\n- `dateTime` - Date and time of the capture in UTC\r\n- `bpm` - Beats Per Minute\r\n- `confidence` - Confidence in accuracy of the reading. 0 = no reading could be made, 3 = highest quality signal.\r\n\r\nExample of three points:\r\n\r\n```\r\n[{\r\n  \"dateTime\" : \"01/25/24 07:00:03\",\r\n  \"value\" : {\r\n    \"bpm\" : 77,\r\n    \"confidence\" : 2\r\n  }\r\n},\r\n{\r\n  \"dateTime\" : \"01/25/24 07:00:08\",\r\n  \"value\" : {\r\n    \"bpm\" : 73,\r\n    \"confidence\" : 3\r\n  }\r\n},\r\n{\r\n  \"dateTime\" : \"01/25/24 07:00:13\",\r\n  \"value\" : {\r\n    \"bpm\" : 72,\r\n    \"confidence\" : 2\r\n  }\r\n}]\r\n```","outputs":[]},{"kind":2,"language":"python","value":"df_list  = []\r\n    \r\nfor json_file in os.listdir(hr_data):\r\n    with open(os.path.join(hr_data, json_file)) as f:\r\n        data = json.load(f)\r\n        \r\n    df = pd.json_normalize(data, sep='_')\r\n    df.columns = df.columns.str.lower().str.replace('value_', '')\r\n    df['datetime'] = ( pd.to_datetime(df['datetime'], format='%m/%d/%y %H:%M:%S', utc=True)\r\n                         .dt.tz_localize(None)\r\n                     )\r\n    df_list.append(df)\r\n        \r\ndf_hr = (\r\n    pd.concat(df_list, ignore_index=True)\r\n      .drop_duplicates(subset=['datetime'], ignore_index=True)\r\n      .rename(columns={'datetime':'timestamp'})\r\n      .sort_values('timestamp')\r\n      .reset_index(drop=True)\r\n)","outputs":[]},{"kind":1,"language":"markdown","value":"## Read GPS Data\r\n\r\nThe GPS data is in GPX format. There are two namespaces we need to use:\r\n\r\n1. The gpx namespace: http://www.topografix.com/GPX/1/1\r\n2. The gte namespace http://www.gpstrackeditor.com/xmlschemas/General/1\r\n\r\nWe'll parse this as a standard XML using `ElementTree`.\r\n\r\nVariables include:\r\n- `name` - Name of the mountain\r\n- `lat` - Latitude\r\n- `lon` - Longitude\r\n- `time` - Timestamp of point with offset\r\n- `hdop` - Horizontal Dilution of Precision. Lower = better horizontal (lat/lon) accuracy.\r\n- `vdop` - Vertical Dilution of Precision. Lower = better vertical (elevation) accuracy.\r\n- `speed` - Speed in km/h. Part of extension in gte namespace.\r\n- `azimuth` - Compass angle. Part of extension in gte namespace.\r\n\r\nA sample of a single capture:\r\n\r\n```\r\n<trk>\r\n  <name>Jan 25, 2024 - Keystone Resort</name>\r\n  <trkseg>\r\n    <trkpt lat=\"39.605675\" lon=\"-105.941414\">\r\n      <ele>2856.891977</ele>\r\n      <time>2024-01-25T09:13:52.453-07:00</time>\r\n      <hdop>19</hdop>\r\n      <vdop>4</vdop>\r\n      <extensions>\r\n        <gte:gps speed=\"1.317580\" azimuth=\"212.300003\"/>\r\n      </extensions>\r\n      </trkpt>\r\n  </trkseg>\r\n</trk>\r\n```","outputs":[]},{"kind":2,"language":"python","value":"gpx_namespace = '{http://www.topografix.com/GPX/1/1}'\r\ngte_namespace = '{http://www.gpstrackeditor.com/xmlschemas/General/1}'\r\n    \r\nall_gps_data = []\r\nfile_list    = [file_name for file_name in os.listdir(gps_data) if file_name.endswith(\".gpx\")]\r\n    \r\nfor gpx_file in file_list:\r\n  with open(os.path.join(gps_data, gpx_file)) as f:\r\n    root = ET.parse(f)\r\n        \r\n    for trkpt in root.findall(f'.//{gpx_namespace}trkpt'):\r\n\r\n        time_elem = trkpt.find(f'{gpx_namespace}time')\r\n        elev_elem = trkpt.find(f'{gpx_namespace}ele')\r\n        gps_elem  = trkpt.find(f'.//{gpx_namespace}extensions/{gte_namespace}gps')\r\n\r\n        row = {\r\n                \"timestamp\": parser.parse(time_elem.text, ignoretz=True),\r\n                \"lat\":       float(trkpt.get(\"lat\")),\r\n                \"lon\":       float(trkpt.get(\"lon\")),\r\n                \"elevation\": float(elev_elem.text),\r\n                \"speed\":     float(gps_elem.get(\"speed\")),\r\n                \"azimuth\":   float(gps_elem.get(\"azimuth\"))\r\n              }\r\n        \r\n        all_gps_data.append(row)\r\n\r\ndf_gps = (\r\n  pd.DataFrame(all_gps_data)\r\n    .drop_duplicates(subset=['timestamp'], ignore_index=True)\r\n    .sort_values('timestamp')\r\n    .reset_index(drop=True)\r\n)","outputs":[]},{"kind":1,"language":"markdown","value":"## Read GPS Metadata\r\n\r\nGPS metadata is within a `.slopes` file, which is a ZIP file. When we unzip it, we read `Metadata.xml`. It has a ton of variables. The main ones we want are:\r\n\r\n- `start` - Start time of activity\r\n- `end` - End time of activity\r\n- `type` - Type of activity (Lift or Run)\r\n- `numberOfType` - Which lift or run number the activity is for (e.g. first run, second lift, etc.)\r\n\r\nAn example of two GPS metadata points:\r\n\r\n```\r\n<Action start=\"2024-01-25 09:13:30 -0700\" end=\"2024-01-25 09:24:29 -0700\" type=\"Lift\" numberOfType=\"1\" …>\r\n<Action start=\"2024-01-25 09:27:35 -0700\" end=\"2024-01-25 09:32:36 -0700\" type=\"Run\" numberOfType=\"1\" …>\r\n```","outputs":[]},{"kind":2,"language":"python","value":"df_list   = []\r\nfile_list = [file_name for file_name in os.listdir(gps_data) if file_name.endswith(\".slopes\")]\r\n    \r\n# .slopes files are just zip files with some CSVs and XML metadata.\r\n# We just want to read Metadata.xml\r\nfor slopes_file in file_list:\r\n    with ZipFile(os.path.join(gps_data, slopes_file), 'r') as zip_file:\r\n        with zip_file.open('Metadata.xml') as xml_file:\r\n            df = pd.read_xml(xml_file, parser='etree', xpath='.//Action')\r\n            \r\n    # Convert start/end to datetimes without the timezone\r\n    df[['start', 'end']] = df[['start', 'end']].map(lambda x: parser.parse(x, ignoretz=True))\r\n    df_list.append(df)\r\n        \r\ndf_gps_meta = (\r\n    pd.concat(df_list, ignore_index=True)\r\n      .sort_values('start')\r\n      .reset_index(drop=True)\r\n)","outputs":[]},{"kind":1,"language":"markdown","value":"# Transform with SAS","outputs":[]},{"kind":2,"language":"python","value":"SAS.df2sd(df_hr, 'hr')\r\nSAS.df2sd(df_gps, 'gps')\r\nSAS.df2sd(df_gps_meta, 'gps_meta')","outputs":[]},{"kind":1,"language":"markdown","value":"Convert timezones","outputs":[]},{"kind":2,"language":"sas","value":"data hr;\r\n    set hr;\r\n\r\n    date = datepart(timestamp);\r\n\r\n    /* Convert to MT */\r\n    if(   '25JAN2024'd <= date <= '28JAN2024'd\r\n       OR '13MAR2025'd <= date <= '15MAR2025'd)\r\n    then timestamp = intnx('hour', timestamp, -7, 'S');\r\n\r\n    /* Convert to ET */\r\n    else if(   '23FEB2024'd <= date <= '24FEB2024'd \r\n            OR '09FEB2025'd <= date <= '10FEB2025'd)\r\n    then timestamp = intnx('hour', timestamp, -5, 'S');\r\n    \r\n    drop date;\r\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"Fast-merge GPS data with GPS Metadata:","outputs":[]},{"kind":2,"language":"sas","value":"data gps_filtered;\r\n    set gps;\r\n    retain start end type numberOfType rc;\r\n\r\n    if(_N_ = 1) then do;\r\n        length type $4.;\r\n\r\n        dcl hash meta(dataset: 'gps_meta', ordered: 'yes');\r\n            meta.defineKey('start', 'end');\r\n            meta.defineData('start', 'end', 'type', 'numberOfType');\r\n        meta.defineDone();\r\n\r\n        dcl hiter iter('meta');\r\n\r\n        call missing(start, end, type, numberOfType);\r\n\r\n        /* Get the first value from the hash table */\r\n        rc = iter.first();\r\n    end;\r\n\r\n    /* If we'ved to a new run, get the next timestamp */\r\n    if(timestamp > end) then rc = iter.next();\r\n\r\n    /* As long as there's a value from the hash table and the\r\n       GPS timestamp is between the start/end points of the\r\n       metadata timestamp, then get the run/lift number and output */\r\n    if(rc = 0 and start <= timestamp <= end) then do;\r\n        if(type = 'Run') then run_nbr = numberOfType;\r\n            else lift_nbr = numberOfType;\r\n        output;\r\n    end;\r\n\r\n    drop start end type numberOfType rc;\r\nrun;","outputs":[]},{"kind":1,"language":"markdown","value":"Merge the closest GPS timestamp to the closest Heartrate timestamp","outputs":[]},{"kind":2,"language":"sas","value":"proc sql;\r\n    create table snowboarding_gps_hr(drop=dif) as\r\n        select round(gps.timestamp) as timestamp format=datetime.2\r\n             , gps.lat\r\n             , gps.lon\r\n             , gps.lift_nbr\r\n             , gps.run_nbr\r\n             , gps.elevation*3.28084 as elevation\r\n             , gps.speed\r\n             , hr.bpm\r\n             , hr.confidence as hr_sensor_confidence\r\n             , abs(round(hr.timestamp) - round(gps.timestamp)) as dif\r\n        from gps_filtered as gps\r\n        left join\r\n             hr\r\n        on   dhms(datepart(gps.timestamp), hour(gps.timestamp), minute(gps.timestamp), 0)\r\n           = dhms(datepart(hr.timestamp), hour(hr.timestamp), minute(hr.timestamp), 0)\r\n        group by calculated timestamp\r\n        having dif = min(dif)\r\n    ;\r\nquit;","outputs":[]},{"kind":1,"language":"markdown","value":"Dedupe and load to CAS","outputs":[]},{"kind":2,"language":"sas","value":"cas; caslib _ALL_ assign;\r\n\r\nproc sort data=snowboarding_gps_hr \r\n          out=casuser.snowboarding_gps_hr\r\n          nodupkey;\r\n    by timestamp;\r\nrun;","outputs":[]}]